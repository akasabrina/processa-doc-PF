{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Default\n",
    "import  pandas   as pd\n",
    "import  numpy    as np\n",
    "import  re\n",
    "import  joblib\n",
    "from    unidecode             import unidecode\n",
    "\n",
    "# Models\n",
    "from    gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from    sklearn.ensemble      import HistGradientBoostingClassifier\n",
    "\n",
    "# Others\n",
    "from    sklearn.model_selection import train_test_split\n",
    "from    sklearn.metrics         import confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingClassifier(max_depth=5, max_iter=1000, random_state=40,\n",
      "                               warm_start=True)\n",
      "\n",
      "Treino:\n",
      "model score train: 99.87%\n",
      "[[3455    0    4]\n",
      " [   0  321    2]\n",
      " [   9    0 7363]]\n",
      "\n",
      "Teste:\n",
      "model score train: 98.05%\n",
      "[[1505    0   14]\n",
      " [   0  122    5]\n",
      " [  73    1 3061]]\n"
     ]
    }
   ],
   "source": [
    "df_texto = pd.read_excel(\"texto_bs2.xlsx\")\n",
    "\n",
    "# divisão de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_texto.texto, df_texto.alvo, test_size=0.3, random_state=42)\n",
    "\n",
    "# pré-processamento\n",
    "substituir = re.compile(r\"(\\W)|(\\S*@\\S*\\s?)|(http\\S+)|(www\\S+)|(\\d+)|(\\\\n)|[ªº_]\")\n",
    "X_train.replace(substituir, \" \", regex = True, inplace=True)\n",
    "X_train = X_train.apply(lambda x: unidecode(x).split())\n",
    "\n",
    "substituir = re.compile(r\"(\\W)|(\\S*@\\S*\\s?)|(http\\S+)|(www\\S+)|(\\d+)|(\\\\n)|[ªº_]\")\n",
    "X_test.replace(substituir, \" \", regex = True, inplace=True)\n",
    "X_test = X_test.apply(lambda x: unidecode(x).split())\n",
    "\n",
    "# doc2vec Treino\n",
    "tagged_doc_train = [TaggedDocument(texto, [i]) for i, texto in enumerate(X_train)]\n",
    "d2v_model = Doc2Vec(tagged_doc_train, vector_size=1500, min_count = 1, epochs = 20, dm=0, window=5)\n",
    "x_treino = np.array([d2v_model.infer_vector(tagged_doc_train[i][0], alpha=0.3, min_alpha=0.07) for i in range(len(tagged_doc_train))])\n",
    "\n",
    "# doc2vec teste\n",
    "tagged_doc_test = [TaggedDocument(texto, [i]) for i, texto in enumerate(X_test)]\n",
    "x_teste = np.array([d2v_model.infer_vector(tagged_doc_test[i][0], alpha=0.3, min_alpha=0.07) for i in range(len(tagged_doc_test))])\n",
    "\n",
    "# Algoritmo de classificação\n",
    "classifier = HistGradientBoostingClassifier(max_iter = 1000, learning_rate=0.1, warm_start=True, random_state=40, max_depth=5)\n",
    "classifier.fit(x_treino, y_train)\n",
    "treino_cls = classifier.predict(x_treino)\n",
    "teste_cls = classifier.predict(x_teste)\n",
    "\n",
    "print(classifier)\n",
    "print(\"\\nTreino:\")\n",
    "print(\"model score train: %.2f%%\" % (classifier.score(x_treino, y_train) * 100.0))\n",
    "print(confusion_matrix(y_train, treino_cls))\n",
    "\n",
    "print(\"\\nTeste:\")\n",
    "print(\"model score train: %.2f%%\" % (classifier.score(x_teste, y_test) * 100.0))\n",
    "print(confusion_matrix(y_test, teste_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após encontrar os melhores resultados crie um modelo usando todos os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingClassifier(max_depth=5, max_iter=1000, random_state=40,\n",
      "                               warm_start=True)\n",
      "model score: 99.89%\n",
      "[[ 4974     0     4]\n",
      " [    0   449     1]\n",
      " [   12     0 10495]]\n"
     ]
    }
   ],
   "source": [
    "# pré-processamento\n",
    "df_texto = pd.read_excel(\"texto_bs2.xlsx\")\n",
    "substituir = re.compile(r\"(\\W)|(\\S*@\\S*\\s?)|(http\\S+)|(www\\S+)|(\\d+)|(\\\\n)|[ªº_]\")\n",
    "df_texto.replace(substituir, \" \", regex = True, inplace=True)\n",
    "df_texto['texto_limpo'] = df_texto.texto.apply(lambda x: unidecode(x).split())\n",
    "\n",
    "# doc2vec\n",
    "tagged_doc = [TaggedDocument(texto, [i]) for i, texto in enumerate(df_texto.texto_limpo)]\n",
    "d2v_model = Doc2Vec(tagged_doc, vector_size=1500, min_count = 1, epochs = 20, dm=0, window=5)\n",
    "\n",
    "x = np.array([d2v_model.infer_vector(tagged_doc[i][0], alpha=0.3, min_alpha=0.07) for i in range(len(tagged_doc))])\n",
    "y = df_texto.alvo\n",
    "\n",
    "# Algoritmo de classificação\n",
    "classifier = HistGradientBoostingClassifier(max_iter = 1000, learning_rate=0.1, warm_start=True, random_state=40, max_depth=5)\n",
    "classifier.fit(x, y)\n",
    "teste_cls = classifier.predict(x)\n",
    "\n",
    "print(classifier)\n",
    "print(\"model score: %.2f%%\" % (classifier.score(x, y) * 100.0))\n",
    "print(confusion_matrix(y, teste_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a pickle in a file\n",
    "joblib.dump(d2v_model, r'model_d2v.pkl')\n",
    "joblib.dump(classifier, r'model_clf2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto_nuvem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
